{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 1. Neural Decision Variables Extraction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 Project Context\nThis notebook implements the decision variables extraction methodology for the ClickDV project - a computational neuroscience analysis linking Poisson click input data from rats to decision variable outputs. This analysis is part of the Brody-Daw lab rotation work."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Objective\nExtract decision variables (DVs) from neural spike data using logistic regression to decode choice from population firing rates. The methodology follows the approach described in \"Brain-wide coordination of decision formation and commitment\" (Bondy et al., 2025 Draft)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2.1 What are Decision Variables?\n\n- **Definition**: 1-dimensional time series DV(t,i) representing neural population \"confidence\" in making a particular choice on trial i at time t\n- **Interpretation**: Positive values indicate evidence for one choice (e.g., rightward), negative values for the alternative choice (e.g., leftward)\n- **Units**: Log-odds of choice probability, equivalent to log(p_right / p_left)\n- **Purpose**: Dimensionality reduction of hundreds of neurons into single time-varying decision signal"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2.2 Key Methodology Features\n\n- Logistic regression with L1 regularization\n- 10-fold stratified cross-validation\n- Geometric mean regularization approach for temporal consistency\n- 50ms Gaussian smoothing of firing rates\n- Trial-by-trial resolution capturing decision formation dynamics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Setup and Dependencies"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3.1 Import Required Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 11\n",
    "})\n",
    "\n",
    "# Suppress sklearn warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SciPy version: {sp.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3.2 Set Reproducibility Parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Analysis parameters (from paper methodology)\n",
    "GAUSSIAN_SIGMA_MS = 50  # Gaussian smoothing kernel standard deviation (ms)\n",
    "SAMPLING_INTERVAL_MS = 50  # Time bin size (ms)\n",
    "TIME_WINDOW = (-0.5, 1.5)  # Analysis window relative to stimulus onset (seconds)\n",
    "CV_FOLDS = 10  # Number of cross-validation folds\n",
    "\n",
    "# Session quality control criteria (from Methods, page 30)\n",
    "MIN_TRIALS = 300  # Minimum trials required for session inclusion\n",
    "MAX_LAPSE_RATE = 0.08  # Maximum allowed lapse rate (8%)\n",
    "\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")\n",
    "print(f\"Gaussian smoothing: {GAUSSIAN_SIGMA_MS}ms standard deviation\")\n",
    "print(f\"Sampling interval: {SAMPLING_INTERVAL_MS}ms\")\n",
    "print(f\"Analysis window: {TIME_WINDOW[0]}s to {TIME_WINDOW[1]}s\")\n",
    "print(f\"Cross-validation: {CV_FOLDS} folds\")\n",
    "print(f\"Quality control: ≥{MIN_TRIALS} trials, ≤{MAX_LAPSE_RATE*100}% lapse rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3.3 Define File Paths and Session Parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "PROJECT_ROOT = Path('/home/ham/SF/Personal/Education/07 Princeton/Rotations/Brody-Daw/ClickDV')\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw' / 'A324' / '2023-07-27'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'notebooks' / 'analysis' / 'outputs'\n",
    "\n",
    "# Session-specific parameters\n",
    "SESSION_ID = 'A324'\n",
    "SESSION_DATE = '2023-07-27'\n",
    "DATA_FILE = DATA_DIR / 'A324_pycells_20230727.mat'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify data file exists\n",
    "if not DATA_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {DATA_FILE}\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Session: {SESSION_ID} ({SESSION_DATE})\")\n",
    "print(f\"Data file: {DATA_FILE}\")\n",
    "print(f\"Data file exists: {DATA_FILE.exists()}\")\n",
    "print(f\"Data file size: {DATA_FILE.stat().st_size / 1024 / 1024:.1f} MB\" if DATA_FILE.exists() else \"N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3.4 Define Core Analysis Functions\n\nThese function signatures will be implemented in subsequent sections of the notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function signatures for the complete pipeline\n",
    "# These will be implemented in subsequent notebook sections\n",
    "\n",
    "def calculate_firing_rates(spike_times: Dict[str, np.ndarray], \n",
    "                          trial_times: np.ndarray, \n",
    "                          time_bins: np.ndarray, \n",
    "                          sigma_ms: float = GAUSSIAN_SIGMA_MS) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate smoothed firing rates using Gaussian convolution.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_times: Dict of {neuron_id: spike_times_array}\n",
    "    - trial_times: Array of trial start times\n",
    "    - time_bins: Time points relative to trial start\n",
    "    - sigma_ms: Gaussian kernel standard deviation (ms)\n",
    "    \n",
    "    Returns:\n",
    "    - firing_rates: Array of shape (n_neurons, n_timepoints, n_trials)\n",
    "    \"\"\"\n",
    "    pass  # Implementation in Section 3\n",
    "\n",
    "def apply_quality_filters(spike_times: Dict[str, np.ndarray], \n",
    "                         quality_metrics: Dict[str, Dict]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Apply neuron quality filters based on waveform characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_times: Dict of {neuron_id: spike_times_array}\n",
    "    - quality_metrics: Dict of quality metrics for each neuron\n",
    "    \n",
    "    Returns:\n",
    "    - filtered_spike_times: Dict with only high-quality neurons\n",
    "    \"\"\"\n",
    "    pass  # Implementation in Section 3\n",
    "\n",
    "def validate_session_quality(choices: np.ndarray, \n",
    "                           n_trials: int, \n",
    "                           lapse_rate: Optional[float] = None) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate session meets quality criteria from paper.\n",
    "    \n",
    "    Parameters:\n",
    "    - choices: Array of behavioral choices\n",
    "    - n_trials: Number of trials\n",
    "    - lapse_rate: Session lapse rate (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - (passes_qc, reason): Boolean and descriptive string\n",
    "    \"\"\"\n",
    "    pass  # Implementation in Section 4\n",
    "\n",
    "def find_optimal_regularization(X: np.ndarray, \n",
    "                               choices: np.ndarray) -> Tuple[float, List[float]]:\n",
    "    \"\"\"\n",
    "    Find optimal regularization using geometric mean approach.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Neural data (n_neurons, n_timepoints, n_trials)\n",
    "    - choices: Binary choices (n_trials,)\n",
    "    \n",
    "    Returns:\n",
    "    - (final_C, individual_lambdas): Optimal regularization parameter and individual values\n",
    "    \"\"\"\n",
    "    pass  # Implementation in Section 5\n",
    "\n",
    "def calculate_decision_variables(X: np.ndarray, \n",
    "                               choices: np.ndarray, \n",
    "                               final_C: float) -> Tuple[np.ndarray, List, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate decision variables using logistic regression.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Neural data (n_neurons, n_timepoints, n_trials)\n",
    "    - choices: Binary choices (n_trials,)\n",
    "    - final_C: Regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "    - (DVs, models, accuracies): Decision variables, fitted models, and accuracies\n",
    "    \"\"\"\n",
    "    pass  # Implementation in Section 6\n",
    "\n",
    "print(\"Function signatures defined\")\n",
    "print(\"Ready to proceed with data loading and analysis\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Load and explore data structure (Section 2)\")\n",
    "print(\"2. Implement data preprocessing (Section 3)\")\n",
    "print(\"3. Apply session quality control (Section 4)\")\n",
    "print(\"4. Implement model training and cross-validation (Section 5)\")\n",
    "print(\"5. Calculate decision variables (Section 6)\")\n",
    "print(\"6. Create visualizations and validation (Sections 7-8)\")\n",
    "print(\"7. Export results and summary (Section 9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Complete\n",
    "\n",
    "This completes the **Introduction & Setup** section of the decision variables extraction notebook. The environment is now configured with:\n",
    "\n",
    "✅ **Libraries imported**: NumPy, SciPy, scikit-learn, Matplotlib, Seaborn  \n",
    "✅ **Reproducibility ensured**: Random seed set to 42  \n",
    "✅ **Parameters configured**: Following paper methodology (50ms smoothing, 50ms sampling, 10-fold CV)  \n",
    "✅ **File paths defined**: Data source and output directories established  \n",
    "✅ **Function signatures**: Complete pipeline structure outlined  \n",
    "\n",
    "**Session Details:**\n",
    "- **Target**: A324 session from 2023-07-27\n",
    "- **Data file**: A324_pycells_20230727.mat \n",
    "- **Analysis window**: -500ms to +1500ms relative to stimulus onset\n",
    "- **Quality criteria**: ≥300 trials, ≤8% lapse rate\n",
    "\n",
    "The notebook is ready for **Section 2: Data Loading & Exploration**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}